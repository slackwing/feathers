# SXIVA Dashboard Project Plan

## Executive Summary

**Goal**: Build an online dashboard at `andrewcheong.com/status` that visualizes personal data collected through SXIVA time-tracking files.

**Architecture**: Static frontend â†’ REST API (on VM) â†’ TimescaleDB (Docker on VM) â† Data sync from local SXIVA files

**Feasibility**: âœ… **This approach is valid and feasible.** The architecture follows standard web app patterns and all components are well-supported.

---

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Local Machine (~/src/feathers/11.sxiv/data/*.sxiva)            â”‚
â”‚                                                                 â”‚
â”‚  1. Run `sxiva` command                                         â”‚
â”‚  2. Check last sync timestamp (via public API)                 â”‚
â”‚  3. Run `sxiva -a` (calculate all files)                       â”‚
â”‚  4. Parse modified .sxiva files                                â”‚
â”‚  5. Sync data to VM database (via API)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â”‚ HTTPS
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Google Cloud VM (andrewcheong.com)                             â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Apache           â”‚  â”‚ Backend API     â”‚  â”‚ Docker Composeâ”‚ â”‚
â”‚  â”‚ /var/www/html/   â”‚  â”‚ (Python/Flask)  â”‚  â”‚               â”‚ â”‚
â”‚  â”‚   status/        â”‚  â”‚                 â”‚  â”‚ - Postgres/   â”‚ â”‚
â”‚  â”‚   (static files) â”‚â—„â”€â”¤ /api/status/*   â”‚â—„â”€â”¤   TimescaleDB â”‚ â”‚
â”‚  â”‚                  â”‚  â”‚ /api/sync/*     â”‚  â”‚ - Liquibase   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ GitHub â†’ bootstrap.sh (curl from repo)                   â”‚  â”‚
â”‚  â”‚   - docker-compose.yml                                   â”‚  â”‚
â”‚  â”‚   - Liquibase changesets                                 â”‚  â”‚
â”‚  â”‚   - API server code                                      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Infrastructure Components

### 1. **Frontend** (`dashboard/static/`)
- **Tech**: Plain HTML/CSS/JavaScript (or lightweight framework like Alpine.js)
- **Purpose**: Static dashboard UI with charts/graphs
- **Deployment**: Manual rsync to VM `/var/www/html/status/`
- **Data source**: Calls backend API routes

### 2. **Backend API** (`dashboard/server/`)
- **Tech**: Python Flask (lightweight, easy to deploy)
- **Purpose**:
  - Serve data queries for dashboard panels
  - Accept data sync from local machine
  - Provide last sync timestamp
- **Routes**:
  - `GET /api/status/last-sync` - Returns last sync timestamp
  - `POST /api/sync/data` - Accepts parsed .sxiva data for a date
  - `GET /api/dashboard/hobby-hours?days=7` - Example query endpoint
  - `GET /api/dashboard/alcohol-timeseries?start=...&end=...`
- **Deployment**: Systemd service on VM, or Docker container

### 3. **Database** (`dashboard/docker/`)
- **Tech**: TimescaleDB (Postgres extension for time-series)
- **Purpose**: Store all SXIVA-derived metrics
- **Management**: Docker Compose + Liquibase for schema versioning
- **Access**: Local to VM only (no public exposure), API server connects via Docker network

### 4. **Deployment/Bootstrap** (`dashboard/deploy/`)
- **Tech**: Bash scripts + Docker Compose
- **Purpose**: Cold-start VM setup, deploy updates, run migrations

---

## Suggested Improvements to Your Plan

### âœ… What's Good
1. **Static frontend + API backend**: Clean separation of concerns
2. **Docker Compose for database**: Easy reproducibility
3. **Liquibase for schema**: Version-controlled database changes
4. **Sync on sxiva startup**: Automatic, seamless integration
5. **Replace-all for each date**: Simplifies sync logic (idempotent)

### ðŸ”§ Suggested Refinements

#### 1. **Deployment Workflow**
**Your idea**: Curl a bootstrap script from GitHub
**Better approach**: Use a small deployment repo structure

```bash
# On VM, one-time setup:
cd /opt
git clone https://github.com/you/sxiva-dashboard.git
cd sxiva-dashboard

# Initial setup
./scripts/bootstrap.sh

# Updates (pull latest changes)
git pull
./scripts/deploy.sh
```

**Why**: Git gives you version control, rollback capability, and doesn't require maintaining a curl-based bootstrap. You can still have `bootstrap.sh` handle initial Docker setup.

#### 2. **Database Access from Local Machine**
**Your concern**: "I don't know how I'd set things up so my computer can speak to a remote VM database"

**Solution**: Don't expose the database directly. Use the API.
- Database only accessible within Docker network on VM
- API endpoint accepts sync data: `POST /api/sync/data`
- API authenticates requests (API key in env var or config file)
- Your local `sxiva` tool sends parsed data via HTTPS to API

**Why**: More secure (database not exposed), easier to manage, standard web pattern.

#### 3. **Modification Timestamps**
**Your concern**: "We might have to fix the calculation step to not overwrite files that haven't changed"

**Current behavior**: The calculator always writes the file (even if unchanged)

**Solution options**:
- **A. Fix calculator**: Only write file if content actually changed (compare before/after)
- **B. Use git timestamps**: Query `git log --format=%at -- path/to/file.sxiva` for last modification
- **C. Track sync state locally**: Keep a local cache of what's been synced (`.sxiva-sync-cache.json`)

**Recommendation**: Option A (fix calculator to not overwrite unchanged files) is cleanest and benefits all users.

#### 4. **Sync Authentication**
**Missing from your plan**: How to authenticate sync requests

**Solution**: Simple API key
```python
# In local sxiva config: ~/.sxivarc or .env
SXIVA_DASHBOARD_API_KEY=your-secret-key-here

# API validates on each sync request
Authorization: Bearer your-secret-key-here
```

#### 5. **Data Sync Strategy**
**Your plan**: Parse .sxiva files, replace all data for that date

**Recommendation**: Use a staging + swap approach
```sql
-- For each date being synced:
BEGIN;
  DELETE FROM daily_stats WHERE date = '2025-01-17';
  INSERT INTO daily_stats (...) VALUES (...);
  UPDATE sync_metadata SET last_sync = NOW();
COMMIT;
```

This ensures atomic updates (all-or-nothing).

---

## Development Phases

### Phase 0: Infrastructure Setup âœ… (COMPLETE)
**Goal**: Get the VM and database foundation ready

- [x] Set up Docker and Docker Compose on VM
- [x] Create `dashboard/` directory structure in this repo
- [x] Create basic Docker Compose file (Postgres/TimescaleDB + Liquibase)
- [x] Test Docker setup locally first
- [x] Deploy to VM and verify database is accessible

**Status**: Complete! TimescaleDB running on VM with Liquibase migrations working.

**Actual time**: ~3 hours

### Phase 1: Database Schema Design
**Goal**: Define what data we're storing

- [x] Document exactly what fields to extract from .sxiva files (you'll provide specs)
- [x] Design database schema (tables, indexes, hypertables)
- [x] Create initial Liquibase changeset (002-daily-summary-schema.xml)
- [x] Test schema creation locally
- [x] Deploy to VM

**Status**: Complete! `daily_summary` table created with JSONB for flexible categories, typed columns for attributes, TimescaleDB hypertable, and GIN index for fast JSONB queries.

**Actual time**: ~2 hours

### Phase 2: Data Sync (Local â†’ VM)
**Goal**: Get data from .sxiva files into the database

#### API Infrastructure âœ… COMPLETE
- [x] Create API endpoint: `POST /api/sync/daily`
- [x] Create API endpoint: `GET /api/status/last-sync`
- [x] Test full sync workflow locally
- [x] Deploy API to VM
- [x] Configure Apache reverse proxy (`andrewcheong.com/status/api/*`)

#### Sync Client Implementation âœ… COMPLETE

**Workflow** (runs automatically when `sxiva` command is executed):

1. **Query last sync from server**
   - [x] Call `GET https://andrewcheong.com/status/api/status/last-sync`
   - [x] Store returned date (e.g., `"2025-01-17"`)
   - [x] If network fails, silently skip sync (fail gracefully)

2. **Recalculate all .sxiva files** (MVP: recalculate everything)
   - [x] Run calculator on all files in data directory
   - [x] Bypass `-a` confirmation prompt (run silently)
   - [x] Don't modify files whose content doesn't change (preserve mtime)

3. **Parse .sxiva files to extract data**
   - [x] Use existing tree-sitter parser (`parser.so`)
   - [x] For each file, extract:
     - [x] Date from filename (e.g., `20250117F.sxiva` â†’ `"2025-01-17"`)
     - [x] Day of week (calculate from date)
     - [x] `{summary}` section â†’ `category_minutes` dict (e.g., `{"bkc": 40, "jnl": 32}`)
     - [x] `{attributes}` section â†’ individual fields:
       - `[sleep] 79 7.0` â†’ `sleep_score=79, sleep_hours=7.0`
       - `[dep] 1 -3 = -1.0` â†’ `dep_min=1.0, dep_max=-3.0, dep_avg=-1.0`
       - Other attributes: `dist`, `soc`, `out`, `exe`, `alc`, `xmx`, `wea`
       - Handle NULL vs 0: empty = NULL, `0` = 0

4. **Filter files to sync**
   - [x] Compare file date with `last_sync_date` from server
   - [x] Only sync files where `file_date >= last_sync_date`
   - [x] Incremental sync working (48 files on first run, 1 file on subsequent runs)

5. **Sync each file to server**
   - [x] Use Python `requests` library (not curl)
   - [x] For each file, call `POST /api/sync/daily` with JSON payload
   - [x] Include `Authorization: Bearer <token>` header
   - [x] Hardcode API token in Python script (MVP)
   - [x] If any request fails, silently skip (we'll sync next time)
   - [x] No retry logic (MVP)

6. **Continue to editor**
   - [x] After sync completes (or fails), open editor as normal
   - [x] User doesn't notice any delay (sync runs silently in background)

**Implementation:**
- [x] Created `tools/sxiva/sync.py` with sync client logic
- [x] Created `tools/sxiva/parser_extractor.py` to parse tree-sitter output
- [x] Integrated sync into `sxiva` CLI command (before editor opens)
- [x] Added `SXIVA_NO_SYNC` and `SXIVA_NO_RECALC` environment variables to disable
- [x] Tested sync from local machine to VM
- [x] Verified data appears correctly in database

**Status**: âœ… COMPLETE! All 48 files synced successfully. Data verified in TimescaleDB.

**Actual time**: ~3 hours

### Phase 3: Backend API (Query Endpoints)
**Goal**: Serve data to the dashboard

- [ ] Design API endpoints based on dashboard panels (you'll provide specs)
- [ ] Implement query endpoints
- [x] Add API authentication/authorization (Bearer token)
- [x] Test API locally
- [x] Deploy to VM
- [x] Configure Apache reverse proxy for API

**Depends on**: You specifying what dashboard panels you want

**Estimated time**: 4-6 hours (once panel specs provided)

### Phase 4: Frontend Dashboard
**Goal**: Build the visual interface

- [ ] Design dashboard layout (you'll provide mockup or specs)
- [ ] Create HTML/CSS structure
- [ ] Implement JavaScript to call API endpoints
- [ ] Add charts/graphs (using Chart.js or similar)
- [ ] Test locally (with API)
- [ ] Deploy to VM (`rsync` to `/var/www/html/status/`)
- [ ] Configure Apache to serve static files

**Depends on**: You specifying what panels and visualizations you want

**Estimated time**: 8-12 hours (once panel specs provided)

### Phase 5: Deployment & Operations
**Goal**: Make it easy to deploy and maintain

- [ ] Create `bootstrap.sh` for cold-start VM setup
- [ ] Create `deploy.sh` for updates
- [ ] Document deployment process
- [ ] Set up backup strategy for database
- [ ] Test cold-start from scratch on clean VM

**Estimated time**: 3-4 hours

---

## Directory Structure

```
11.sxiv/
â”œâ”€â”€ dashboard/
â”‚   â”œâ”€â”€ README.md                  # Dashboard-specific docs
â”‚   â”œâ”€â”€ static/                    # Frontend files (rsync to VM)
â”‚   â”‚   â”œâ”€â”€ index.html
â”‚   â”‚   â”œâ”€â”€ styles.css
â”‚   â”‚   â”œâ”€â”€ app.js
â”‚   â”‚   â””â”€â”€ assets/
â”‚   â”œâ”€â”€ server/                    # Backend API
â”‚   â”‚   â”œâ”€â”€ app.py                 # Flask application
â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”‚   â”œâ”€â”€ sync.py           # Sync endpoints
â”‚   â”‚   â”‚   â””â”€â”€ dashboard.py      # Query endpoints
â”‚   â”‚   â”œâ”€â”€ models/                # Database models
â”‚   â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”‚   â””â”€â”€ config.py
â”‚   â”œâ”€â”€ docker/                    # Docker infrastructure
â”‚   â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â”‚   â”œâ”€â”€ postgres/
â”‚   â”‚   â”‚   â””â”€â”€ Dockerfile
â”‚   â”‚   â””â”€â”€ liquibase/
â”‚   â”‚       â”œâ”€â”€ Dockerfile
â”‚   â”‚       â””â”€â”€ changelog/
â”‚   â”‚           â””â”€â”€ db.changelog-master.xml
â”‚   â”œâ”€â”€ deploy/                    # Deployment scripts
â”‚   â”‚   â”œâ”€â”€ bootstrap.sh          # Cold-start setup
â”‚   â”‚   â”œâ”€â”€ deploy.sh             # Update deployment
â”‚   â”‚   â””â”€â”€ backup.sh             # Database backup
â”‚   â””â”€â”€ docs/                      # Additional documentation
â”‚       â”œâ”€â”€ API.md                # API documentation
â”‚       â””â”€â”€ SCHEMA.md             # Database schema docs
â”œâ”€â”€ tools/sxiva/
â”‚   â””â”€â”€ sync.py                    # Data sync client (NEW)
â””â”€â”€ DASHBOARD_PLAN.md              # This file
```

---

## Technical Decisions & Rationale

### Why TimescaleDB over plain Postgres?
- **Time-series optimization**: Fast queries on date ranges
- **Automatic partitioning**: Better performance as data grows
- **Compression**: Saves disk space for historical data
- **Continuous aggregates**: Pre-computed rollups (e.g., weekly sums)
- **Still Postgres**: Can use all standard Postgres features

### Why Flask over other frameworks?
- **Lightweight**: Easy to deploy, minimal dependencies
- **Python**: Matches existing SXIVA tooling
- **Simple**: No ORM complexity needed for simple queries
- **Fast to develop**: Get API up quickly

### Why static frontend over React/Vue?
- **Simplicity**: No build step, no node_modules
- **Fast**: Direct HTML/CSS/JS loads instantly
- **Easy deployment**: Just rsync files
- **Can upgrade later**: If needed, add a framework later

### Why API key authentication?
- **Simple**: No OAuth complexity
- **Sufficient**: You're the only user
- **Secure enough**: HTTPS + secret key
- **Easy to rotate**: Just update config and redeploy

---

## Security Considerations

1. **Database**: Not exposed to internet, only accessible from API container
2. **API**: HTTPS only (via Apache), requires API key for writes
3. **API Key**: Stored in environment variables, not in code
4. **Frontend**: Static files, no sensitive data hardcoded
5. **VM**: Standard security (firewall, SSH keys, regular updates)

---

## Unknowns / Need Your Input

Before proceeding, I need you to specify:

### 1. **Data to Extract from .sxiva Files**
What specific fields/values should we parse and store?
- Summary time totals per category? (we have this)
- Attribute values? (sleep hours, depression score, etc.)
- Medication data from `[med]` lines?
- Individual time block data? (probably not needed, just aggregates)
- Date and day of week?
- Other metadata?

### 2. **Dashboard Panels/Visualizations**
What do you want to see? Examples:
- "7-day sum of hours spent on hobbies" - Bar chart? Line chart?
- "Time-series of drinks (alcohol) each day" - Line chart over how many days?
- "Sleep hours trend over last 30 days"
- "Category breakdown pie chart for this week"
- "Streaks: days with meditation, exercise, etc."
- Others?

### 3. **Database Schema Details**
Once I know what data to extract, I can design:
- Table structure
- Indexes
- Hypertables (for TimescaleDB)
- Sample queries for your panels

---

## Next Steps

1. **Review this plan**: Does the architecture make sense? Any concerns?
2. **Provide specifications**:
   - What data to extract from .sxiva files
   - What dashboard panels/visualizations you want
3. **Choose starting phase**: Want to start with infrastructure (Phase 0) or wait until specs are complete?

---

## Estimated Total Time

- **Infrastructure + Database**: 4-7 hours
- **Data Sync**: 6-8 hours
- **Backend API**: 4-6 hours (depends on complexity of panels)
- **Frontend**: 8-12 hours (depends on number of panels)
- **Deployment**: 3-4 hours

**Total**: ~25-37 hours of development time

**Phased approach**: We can build this incrementally:
- Phase 0-2: Get data flowing (basic infrastructure)
- Phase 3-4: Build one panel at a time
- Phase 5: Polish and deployment

---

## Questions?

- Does this architecture make sense?
- Should we simplify or expand any part?
- Ready to define data extraction specs and dashboard panels?
- Want to start with Phase 0 (infrastructure setup)?
