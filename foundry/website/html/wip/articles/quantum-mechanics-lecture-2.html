<?php

$id = 0;
$version = '0.1';
$published = 'N/A';
$updated = 'N/A';
$title = 'Quantum Mechanics: Lecture 2, a re-telling';
$content = <<<'HEREDOC'

$\require{unicode}$

<div class="intro">

This article is an alternative presentation of Lecture 2 of _Quantum Mechanics: The Theoretical Minimum_ by Leonard Susskind and Art Friedman. This means you should be able to read Lecture 1, read this article, then continue on with Lecture 3. This re-telling is meant to be more accessible, but not at the cost of depth‚Äîin fact challenging the original in that aspect.

Susskind stresses a careful read of this lecture, but it's written in a way that asks the student to take numerous points on faith, _e.g._ because such and such is convention, or because so and so _just is_, or because _we'll see later_. Considering the depth of the subject, that's unavoidable. But how "careful" can a student feel about their reading, having swallowed ideas whole? Not that I'm saying Susskind _et al._ failed to explain enough. An extraordinary amount of effort of planning and revisions went into presenting _just_ enough. It must have been painful, whittling away sentences that were illuminating and exciting but not essential.

But a book can be sliced in more than one way. Here's a way that exchanges one part optimality for two parts empathy. Susskind _et al._ drew the map; I'm thinking about who walks the roads, and putting up signs. <!-- Let's get started. -->

<!--
This article is an alternative presentation of Lecture 2 of _Quantum Mechanics: The Theoretical Minimum_ by Leonard Susskind and Art Friedman. This means you should be able to read Lecture 1, read this article, then continue on with Lecture 3. The hope is that this re-telling will be more accessible, more clarifying, and for some even more powerful than the original. Susskind stresses a careful read of this lecture, but it's written in a way that asks the student to take numerous points on faith, _e.g._ because such and such is convention, or because so and so _just is_, or because _we'll see later_. Considering the depth of the subject, that's unavoidable. But how "careful" can a student feel about their reading, having swallowed ideas whole? So then, am I saying Susskind _et al._ failed to explain enough in these lectures? No. An extraordinary amount of effort of planning and revisions went into presenting _just enough_. It must have been painful, whittling away sentences that were illuminating and exciting _but not essential_. What I offer in this article are not the pieces shed away, but extra mortar and braces for the pieces that stayed. I also re-tell the story with **"empathetic annotations"** to extend a hand to those struggling or discouraged, but also to fortify any level of reader's understanding with callbacks and a second perspective. Susskind _et al._ drew the map; I'm thinking about who walks the roads, and putting up signs.
-->

</div>

![](https://i.snap.as/fkehLQyH.webp)

<h2>1. The situation <s>doesn't</s> does add up</h2>

<blockquote>"Let's begin by labeling the possible spin states along the three coordinate axes. If ùíú is oriented along the z-axis, the two possible states that can be prepared correspond to $\sigma_z=\pm1$. Let's call them up and down and denote them by ket-vectors $\ket{u}$ and $\ket{d}$."</blockquote>

First, drop all $\sigma$ symbols from mind‚Äîthey're not needed and we won't see them again for a while. Second, recall ùíú the **apparatus**‚Äîa box you can't look inside, but can turn (**orient**) in a certain direction and read a measurement of either $+1$ or $-1$‚Äîat which point you can say you've **prepared** ùíú in that direction or its opposite, respectively. That's a lot of English words to describe that situation, so we're going to come up with some symbols to represent each possible situation:

<div class="declaration">
  <p class="declaration-type">declaration</p>
  <p>
    $\ket{u}$ will mean ùíú was oriented along the z-axis and read $+1$ (up).<br/>
    $\ket{d}$ will mean ùíú was oriented along the z-axis and read $-1$ (down).<br/>
    $\ket{r}$ will mean ùíú was oriented along the x-axis and read $+1$ (right).<br/>
    $\ket{l}$ will mean ùíú was oriented along the x-axis and read $-1$ (left).<br/>
    $\ket{o}$ will mean ùíú was oriented along the y-axis and read $+1$ (out).<br/>
    $\ket{i}$ will mean ùíú was oriented along the y-axis and read $-1$ (in).
  </p>
  <p><strong>Whenever you feel lost, come back here to re-read this and re-ground yourself.</strong></p>
</div>

Which orientation's on which axis and whether it's the $+1$ or the $-1$ is arbitrary. A way to remember our version is: Imagine holding up an open book (or phone) in front of you at eye level like you're going to read it; the x-axis naturally spans left and right; the y-axis goes <em>into</em> the pages (the screen) or pops <em>out</em> toward you; the z-axis goes up toward the ceiling or down toward the floor.

Anyway, you might wonder why or how these situations are represented by **ket vectors**. Forget you ever learned anything about ket vectors. Pretend these are brand new symbols we've just invented as shorthand for the six situations.

Sometimes, we may want to refer to a situation but not specifically. For example, we may not know what the situation is. Just as we introduce $x$ in algebra to refer to an unknown quantity, let's invent a symbol to do the same for situations: $\ket{X}$, let's say. But just as you can use any letter in algebra‚Äî$y$, $z$, $a$, $b$‚Äîyou can use any letter here too‚Äî$\ket{Y}$, $\ket{Z}$, $\ket{A}$, $\ket{B}$. Let's keep these upper-cased though so as not to confuse them with the lower-cased, "known" situations $\ket{u}$, $\ket{d}$, $\ket{l}$, $\ket{r}$, $\ket{i}$, $\ket{o}$. So now that we've got ourselves some symbols, we're going to present a [strange rule](strange-rule.html):

<div class="declaration">
  <p class="declaration-type">declaration</p>
  <p>You can treat any situation‚Äîor rather, a symbol representing any situation‚Äîas if it were a quantity, whether the situation is "known" like $\ket{u}$ or "unknown" like $\ket{X}$.</p>
</div>

We are not specifying what kind of "quantity"‚Äîreal or complex, scalar or vector. Just that whatever it is, you can **add** and **multiply** this quantity with other quantities. For example,

$$2\ket{u}+3\ket{d}$$

But what does it mean to double the $\ket{u}$ situation or triple the $\ket{d}$ situation? What does it mean to add up situations? ü§∑ Consider it a [strange rule](strange-rule.html) for now.

Next the lecture says,

<blockquote>"The idea that there are no hidden variables has a very simple mathematical representation: the space of states for a single spin has only two dimensions. This point deserves emphasis: <i>All possible spin states can be represented in a two-dimensional vector space.</i>"</blockquote>

<!-- ![](https://i.snap.as/FGYgmjf6.gif) -->

The tone is that this fact‚Äîwhatever it means‚Äîis self-evident. But it's hardly that.<sup>1</sup> Because of this confusion, only a fraction of readers will recognize that the next sentence is actually a more concrete reiteration of the same fact:

<blockquote>We could, somewhat arbitrarily, choose $\ket{u}$ and $\ket{d}$ as the two basis vectors and write <i>any</i> state as a linear superposition of these two. We'll adopt that choice for now.</blockquote>

So instead, many readers will pocket the first part as a failure in their understanding, and consider this second part the start of a new argument. This wouldn't have been undermining at all, had the lecture not been so stingy as to offer just one additional sentence: **The next three subsections of the lecture‚Äî2.3, 2.4, and 2.5‚Äîwill be spent building up to the statements above.** So there was no reason to mystify readers then bulldoze ahead with no hint of where we're going. So relax‚Äîyou haven't missed anything. Let's get to the same destination at our own pace.

<h2>2. Getting into situationships</h2>

<blockquote>Let's use the symbol $\ket{A}$ for a generic state. We can write this as an equation, $$\ket{A}=\alpha_u\ket{u}+\alpha_d\ket{d}\small\textrm{,}$$ where $\alpha_u$ and $\alpha_d$ are the components of $\ket{A}$ along the basis directions $\ket{u}$ and $\ket{d}$."</blockquote>

The lecture is making a bold proposition that $\ket{r}$, $\ket{l}$, $\ket{o}$, $\ket{i}$, and in fact _any_ situation, $\ket{A}$, can be expressed as an arithmetic formula of just the $\ket{u}$ and $\ket{d}$ situations alone ($\alpha_u$ and $\alpha_d$ being yet-to-be-determined constants). Intuitively, this might feel impossible. No matter how "up" or how "down" you go, or any combination of the two, how could that result in going right, left, out, or in? But that's a misinterpretation of the proposition. Remember that we're talking about situations, not movement in spatial directions. So it's not saying that a "left" movement can be expressed in terms of "up" and "down" movements. It's saying that a "left" _situation_ can be expressed in terms of "up" and "down" _situations_. Which might be slightly more digestable, but still, how does this "arithmetic formula" work? What are we calculating on the left- and right-hand sides of that equation such that they can be said to "equal" one another? In other words, how do we **do math with** and **compare** situations? Let's begin with the latter.

When we compare anything to anything, we implicitly compare a **property** (or **set of properties**) that the things have. For example, "Are these 2 oranges the same?" might invite the answer "Yes" if the properties we're interested in are color, overall shape, approximate size, and expected taste. But it may also invite the answer "No" if we are looking at their _exact_ sizes, weights, or coloration. So, 2 things can be the same or different depending on the chosen property or properties. That choice, in turn, directly relates to the _purpose_ of comparison‚Äî_Why_ are we comparing these oranges? The oranges may be the "same" if we're just making orange juice, but "different" if we're sorting them for the market, or ringing them up by weight. **We say 2 things are the same if they share _properties_, usually relevant to some _purpose_.**

Properties are much harder to come by in the sub-atomic world. We don't have sub-atomic taste buds, most particles don't emit electromagnetic waves in the visible spectrum (don't have a color), and some particles don't even show themselves (exhibit a property that we can detect) without colliding other particles into each other near the speed of light. So properties we _can_ detect in the sub-atomic world can sound rather abstract or contrived‚Äîcase in point: our apparatus ùíú that detects "situations." But tangible or intangible, real or imagined (taste is a hallucination in your brain), **a property is a property if comparing it serves a purpose, and serves it consistently**.

Coming back to the original question‚Äî<i>how do we **compare** situations</i>‚Äînow you can understand when we say: It comes down to the _property_ or _properties_ we want to compare. We know properties that an orange can have, but what properties can a "situation" have? Briefly recall what our "situation" was: It was <strong>the reading of a $+1$ or $-1$ on an apparatus ùíú.</strong> So it's not a physical thing‚Äîit's, literally, a _situation_ in the colloquial sense, not just in our specialized sense. So let's think: What properties can situations _in general_ have? Think about a friend's birthday or a foggy morning or two stars colliding. The most obvious property is: It _happened_ or it _didn't_. If we were to use this property for comparison, then we could say that the friend's birthday, foggy morning, and collision of stars are all the "same," in that they all happened. But such a comparison isn't useful to us‚Äîit serves no _purpose_. _Our_ purpose is to study a certain feature of quantum mechanics, and to do so, the property we're looking for is about to reveal itself.

Let's go back to build on the situation. Having **prepared** ùíú in one of the situations $\ket{u}$, $\ket{d}$, $\ket{r}$, $\ket{l}$, $\ket{o}$, $\ket{i}$, now suppose we **re-orient** ùíú on a _different_ axis, _i.e._ turn it 90¬∞ in some direction. What will a measurement yield now, $+1$ or $-1$? The answer, known through real experiments on real devices which our hypothetical device ùíú is based off of: **It appears to be unpredictable.** No matter how identically we perform the setup. Some might argue there are hidden variables at play. Fine then, but either way _we today_ cannot discern a predictable mechanics. Not for the specific _value_ of the measurement at least. But over many runs, something peculiar begins to show. The device appears to measure $+1$ 50% of the time, and $-1$ the other 50% of the time.




And _that_ right there‚Äîthe 2 

_Our_ purpose is to study a certain feature of quantum mechanics as seen through a hypothetical device‚Äîthe apparatus ùíú. This device‚Äîmodeled on a real device of course‚Äîhas a peculiar behavior that gives rise to an interesting property we'd like to study‚Äîalso the property we can use to compare situations. It's simply this: If we prepare one of the situations , then **re-orient** ùíú on a _different_ axis, then, no matter how identically we perform the prior steps, taking 


. _If_ you were to take a measurement at this point, you'll read $+1$ half the time, and $-1$ the other half of the time, <em>despite setting up the same starting situation and re-orienting the device exactly the same way every time</em>. In other words, the measurement of the re-oriented situation appears to be **non-deterministic**. Let's create a new notation to depict this "re-oriented situation" so it's easier to talk about:

<div class="declaration">
  <p class="declaration-type">declaration</p>
  <p>If we prepare the apparatus ùíú in, say, the situation $\ket{u}$ (oriented on what we're calling the z-axis), and then re-orient it on, say, the x-axis, facing the direction we'd have called $\ket{r}$, then we write $$\braket{r|u}$$ as shorthand for this entire situation.</p>
</div>

<!-- It's a little strange that it reads backwards (the right happens first, then the left), but perhaps we can take a grander stance and say English is what's backwards, not the notation. In any case, it's a very compact representation of several steps, which makes it as convenient as it is dangerous‚Äîmore prone to over-generalization or dissociation from its original meaning. But we'll keep this in check together. -->

Indeed $\braket{r|u}$ is a situation, but one that we have not yet measured. If we _were_ to take a measurement, and it reads $+1$, then the situation would just become a plain $\ket{r}$. If $-1$, then just a plain $\ket{l}$. But as it remains unmeasured, $\braket{r|u}$ is a situation with a 50% **probability** of measuring $+1$ and 50% probability of measuring $-1$. Of course, this means $\braket{l|u}$ is also a situation with a 50% probability of measuring $+1$ and 50% probability of measuring $-1$.

You may have noticed some rather tiring duplicity in these statements. If $\braket{X|u}$ has an $x\\%$ probability of measuring $+1$, then there's no need to mention the probability of measuring $-1$, which is simply $(100-x)\\%$. Similarly, if $\ket{\bar{X}}$ points opposite $\ket{X}$, then $\braket{\bar{X}|u}$'s probabilities are also inferrable, as $(100-x)\\%$ and $x\\%$. So going forward, let's economize and only speak of one of each pair of situations, and one of each pair of measurements ($+1$).

So then all we need to say is that $\braket{r|u}$'s probability of measuring $+1$ is 50%. Let's move onto the other axes. $\braket{o|u}$

But they aren't necessarily the same always. It's just that so far we've only been dealing with "simple" orientations, aligned with the axes themselves.

Similarly though, re-orienting ùíú toward $\ket{o}$ or $\ket{i}$, the situations are the same: $\braket{o|u}$ and $\braket{i|u}$ are _also_ both situations with a 50% probability of measuring $+1$ and 50% probability of measuring $-1$.

What re-orientations are left? Just $\ket{u}$ and $\ket{d}$. Though, $\braket{u|u}$ isn't really a re-orientation. Since ùíú already measured $+1$ to give us the situation $\ket{u}$, $\braket{u|u}$ is a situation with a 100% probability of measuring $+1$ again, and therefore 0% probability of measuring $-1$. And $\braket{d|u}$, although an actual re-orientation, due to $\ket{d}$'s relationship with $\ket{u}$ is a situation with a 0% probability of measuring $+1$, and 100% probability of measuring $-1$.

Let's summarize 



<!-- Later, indistinguishability of o/i form r/l, is part of it. -->



_All that right there, but together. That_ is the _property_ of a situation we are interested in‚Äîinterested in studying, interested in _comparing situations with_: **given a situation, the _set of probabilities_ of reading a $+1$ or $-1$ for _all re-orientations_ of the apparatus ùíú.**



As you may have already noticed, it's redundant to speak of the probability of reading $-1$ since 
<strong>It's the _set of probabilities_ of reading a $+1$ or $-1$.</strong> We have 6 orientations‚Äîeach of the directions, so that's 6 probabilities:

 1. $\braket{r|u}$ has a 50% chance of reading $+1$.
 2. $\braket{l|u}$ has a 50% chance of reading $+1$.

 Had we chosen the y-axis instead of the $x$, it'd be the same thing:

 3. $\braket{o|u}$ has a 50% chance of reading $+1$.
 4. $\braket{i|u}$ has a 50% chance of reading $+1$.

If we _don't_ re-orient ùíú, or if we re-orient it the opposite way, we've already taken a measurement on the z-axis, so:

 5. $\braket{u|u}$ has a **100%** chance of reading $+1$.
 6. $\braket{d|u}$ has a **0%** chance of reading $+1$.

What should we call this property of a situation‚Äîthis set of 6 probabilities? Let's give <strong>situationship</strong> a try. $\braket{r|u}$'s **situationship** then, fully written out, is $\\{50\\%, 50\\%, 50\\%, 50\\%, 100\\%, 0\\%\\}$. Finally we can answer the question:

<div class="declaration">
  <p class="declaration-type">declaration</p>
  <p>For <em>our</em> purposes, we <strong>compare</strong> situations‚Äî<i>i.e.</i> say that 2 situations are the same or not‚Äîby their <strong>situationship</strong>. <em>One</em> way to represent a situation's situationship is as a collection of 6 probabilities, $\{r\%, l\%, o\%, i\%, u\%, d\%\}$. So if we take 2 situations and compare each of their 6 probabilities and they are all equal, then we consider them the <strong>same</strong>; otherwise not.</p>
</div>

We won't be using the 6-probability notation for long, because it's hard to **do math with**. Let's dive into that part of the question, next.

<h2>3. The most convenient situationship</h2>

You may have wondered if we _really_ needed 6 probabilities, since if it's not _up_, then it's _down_, and vice versa, on each axis. In other words, the probabilities have to add up to 100% on each axis, so if we know one probability then we know the other. That's correct‚Äîwe could have simply said $\\{50\\%, 50\\%, 100\\%\\}$ for just right, out, and up. This is an example of correcting ourselves from using 6 **degrees of freedom** to 3.

But would you be surprised to learn that in fact, we only need **2** degrees of freedom to express a situationship?

Recall the example of doing arithmetic with situations:

$$2\ket{u}+3\ket{d}$$

If asked to think of a way to **do math with** this, the most obvious idea is to replace situations with numbers, _e.g._

$$2(5)+3(7)$$

But it'd be natural to sense that this can't be it. (Computing the above, what would $31$ mean?) A step in the right direction might be to replace situations with their situationships, _e.g._

$$2\\{50\\%, 50\\%, 100\\%\\}+3\\{50\\%, 50\\%, 0\\%\\}$$

By the way, we never really discussed it, but in the way _we_ are comparing situations‚Äîvia their situationships‚Äî$\ket{u}$ and $\braket{u|u}$ are the same, and $\ket{d}$ and $\braket{d|d}$ are the same. Think of it this way: If someone else were to prepare ùíú without our supervision, would we be able to tell whether they just prepared $\ket{u}$ and left it alone, or prepared $\ket{u}$ then measured again in that same orientation? No, we wouldn't, no matter how many times they re-measured. So more generally speaking, **in the way _we_ are comparing situations,** $\ket{X}=\braket{X|X}$. **This is _not_ true in Susskind's lectures‚Äî$\ket{X}\neq\braket{X|X}$!** Because they are not using situationships for comparison. They are using a different property called **probability amplitudes**. We will end up in the same place‚Äîwe're just taking switchbacks to get there rather than scaling the mountain head-on. We get to enjoy more nature our way.

Back to the expression $2\\{50\\%, 50\\%, 100\\%\\}+3\\{50\\%, 50\\%, 0\\%\\}$ though‚Äîhow would we compute it? Distribute and add‚Äî$\\{250\\%, 250\\%, 200\\%\\}$?‚Äîthat makes no sense. A weighted sum‚Äî$\\{50\\%, 50\\%, 40\\%\\}$? We are of course playing around, but not for no reason. (If you've read about [strange rules](strange-rule.html) then you already know what we're doing, but for everyone else, there's a need to acclimate ourselves to the arbitrariness of mathematical systems, before we move on.)

There's a third way to do arithmetic with an expression like $2\ket{u}+3\ket{d}$. Instead of _replacing_ the situations, what if we used them like **dimensions**‚Äîlike we use $m$ (meters) and $s$ (seconds) in physics? And instead of plain numbers like $2$ and $3$, we used situationships as their coefficients?

$$\\{50\\%, 50\\%, 100\\%\\}\ket{u}+\\{50\\%, 50\\%, 0\\%\\}\ket{d}$$

$$\ket{r}=\\{50\\%, 50\\%, 100\\%\\}\ket{u}+\\{50\\%, 50\\%, 0\\%\\}\ket{d}$$

$$\\{50\\%\bra{r}, 50\\%\bra{o}, 100\\%\bra{u}\\}\ket{u}+\\{50\\%\bra{r}, 50\\%\bra{o}, 0\\%\bra{u}\\}\ket{d}$$

$$\\{0.5\bra{r}, 0.5\bra{o}, 1.0\bra{u}\\}\ket{u}+\\{0.5\bra{r}, 0.5\bra{o}, 0.0\bra{u}\\}\ket{d}$$

$$\\{0.5=\bra{r}, 0.5=\bra{o}, 1.0=\bra{u}\\}\ket{u}+\\{0.5=\bra{r}, 0.5=\bra{o}, 0.0=\bra{u}\\}\ket{d}$$

$$\ket{r}=\\{0.5, 0.5, 1.0\\}\ket{u}+\\{0.5, 0.5, 0.0\\}\ket{d}$$

$$\\{x, y, z\\}\ket{r}=\\{0.5, 0.5, 1.0\\}\ket{u}+\\{0.5, 0.5, 0.0\\}\ket{d}$$

$$\\{1.0, 0.5, 0.5\\}\ket{r}=\\{0.5, 0.5, 1.0\\}\ket{u}\bigoplus\\{0.5, 0.5, 0.0\\}\ket{d}$$

<div class="wip">

<b>log</b>

- 11/6 - Two paths.
    - (1) Accept that this is a strange rule‚Äîthe whole linear superposition and probability amplitude thing.
    - (2) Continue trying to justify how one might arrive there.
        - I have a new lead. Go back to the equations from 10/30, but like this:
            $$\begin{alignat}{2}
            \ket{u}=&\begin{bmatrix} 0.5 \\\\ 0.5 \\\\ 1.0 \end{bmatrix}\\\\
            \ket{d}=&\begin{bmatrix} 0.5 \\\\ 0.5 \\\\ 0.0 \end{bmatrix}\\\\
            \ket{r}=&\begin{bmatrix} 1.0 \\\\ 0.5 \\\\ 0.5 \end{bmatrix}=f(\ket{u}, \ket{d}, \alpha)
            \end{alignat}$$
        - No wait, let's do something else.
        - We are already giving the givens of what orientations and re-orientations will (probabilistically) measure as. We can additionally give the continuous spherical data WLOG. We can say, look, it boils down to a _direction_, which requires 2 degrees of freedom to specify for 3-space.
        - Why is the $\ket{d}$ factor even needed in the textbook's equation?
        - Think:
            $$\cos^{2}\theta+\cos^{2}\phi$$
        - And then: Imagine the percentage component coming from $\theta$ was in real units, while the percentage component coming from $\phi$ was in imaginary units. So more like:
            $$\cos^{2}\theta+i\cos^{2}\phi$$
        - (hours of thought)
        - Leftoff: Vague notions about the components of the %, putting $\phi$'s component in the imaginary domain. Next maybe: Can I take 2 probability components and get the exact direction?
- 10/31 - What are my competing thoughts right now?
    - The spin operator $\sigma$ in Lecture 3 operates on a situation. Our "basis" situations are the eigensituations of the spin operator, a.k.a. the spin operator doesn't change the situation.
    - We want to get to representing situations as a column vector of size 2, **with implied dimensions‚Äîa hidden $\begin{bmatrix} \ket{u} \ket{d}\end{bmatrix}$.**
    - Two questions. (1) How does a coefficient on $\ket{d}$ help capture the 2nd dimension, when the 2nd coefficient is actually determined by the 1st already? (Is this where the magic of imaginary numbers come in?) <span style="color:rgb(0,0,128)">11/5 - I think the coefficient does <em>not</em> help if they simply had to sum to 100. But because their <em>squares</em> have to sum to 100, there is an ambiguity‚Äîa choice between 2 numbers.</span> (2) Why or how does squaring enter the picture?
        - Regarding the first question, if you determine the 1st coefficient and try to calculate the 2nd, introducing a square root is what creates the bifurcation to two options for the 2nd coefficient ($\pm$). But this seems like a "trick"‚Äîwhy are things this way? Well, we know we need at minimum 2 degrees of freedom to map to 4 outputs, right? But if the coefficients are related to probabilities, and probabilities have to add up to 1, then we essentially lose the degree of freedom. _Unless_ there is a choice on the complex plane! So **the complex plane is _one_ way to add a degree of freedom**. Do we basically just need _any_ 2-vector representation, and everything that follows (_e.g._ machines) take that choice as a given? Well, what the lecture has done is, it's created a 2-vector representation with certain _relationships_, _e.g._ the sum of squares of the 2 numbers is 1.0 (which just "binds" the opposite direction to the main) (and this works continuously!), and the 2 numbers are the same (magnitude at least) when measuring a 90 degree rotated situation.
        - Another picture. We need a _map_ from _some domain_ to the 6 situations, while respecting the constraints (above). The _some domain_ needs 2 degrees of freedom, so the complex domain is a choice.
- 10/31 - Oh my god, how did I not see this. Going from 1 input to 4 outputs would at minimum require 2 parameters! The math is just all different ways of "implementing" the choice.
- 10/30 - Unless we use a matrix. Oh, whoa. Does this mean a matrix's columns (or rows, not sure) compacts dimensions (or degrees of freedom) into a 1-d looking thing? Let's try a matrix representation: $$
\begin{alignat}{2}
\ket{r}=\begin{bmatrix} 1.0 \\\\ 0.5 \\\\ 0.5 \end{bmatrix}=f(\begin{bmatrix} 0.5 \\\\ 0.5 \\\\ 1.0 \end{bmatrix}&, \alpha)\\\\
\ket{l}=\begin{bmatrix} 0.0 \\\\ 0.5 \\\\ 0.5 \end{bmatrix}=f(\begin{bmatrix} 0.5 \\\\ 0.5 \\\\ 1.0 \end{bmatrix}&, \alpha)\\\\
\ket{o}=\begin{bmatrix} 0.5 \\\\ 1.0 \\\\ 0.5 \end{bmatrix}=f(\begin{bmatrix} 0.5 \\\\ 0.5 \\\\ 1.0 \end{bmatrix}&, \alpha)\\\\
\ket{i}=\begin{bmatrix} 0.5 \\\\ 0.0 \\\\ 0.5 \end{bmatrix}=f(\begin{bmatrix} 0.5 \\\\ 0.5 \\\\ 1.0 \end{bmatrix}&, \alpha)\\\\
\end{alignat}$$
- 10/30 - But defer that for a sec. How can I get $\ket{o}$ and $\ket{i}$? We'd need a 2nd coefficient I think.
- 10/30 - But you have to do all this in terms of rotations, to end up with a continuous result...
- 10/30 - Let's try defining a function... $$f(\sigma, \alpha)=\\{\sigma_1+\alpha, \sigma_2, 0.5\\}$$
- 10/30 - The $\ket{d}$ side is implied from the other, so: $$\begin{alignat}{2}
\ket{r}=\\{1.0, 0.5, 0.5\\}=f(\\{0.5, 0.5, 1.0\\}&, \alpha_r)\\\\
\ket{l}=\\{0.0, 0.5, 0.5\\}=f(\\{0.5, 0.5, 1.0\\}&, \alpha_l)\\\\
\ket{o}=\\{0.5, 1.0, 0.5\\}=f(\\{0.5, 0.5, 1.0\\}&, \alpha_o)\\\\
\ket{i}=\\{0.5, 0.0, 0.5\\}=f(\\{0.5, 0.5, 1.0\\}&, \alpha_i)\end{alignat}$$
- 10/29 - No, these are unnecessary because those kets are already given.
- 10/29 - What are the rest of the equations? $$\begin{alignat}{2}
\ket{r}+\ket{l}&=\\{1.0, 1.0, 1.0\\}\\\\
\ket{o}+\ket{i}&=\\{1.0, 1.0, 1.0\\}\\\\
\ket{u}+\ket{d}&=\\{1.0, 1.0, 1.0\\}\end{alignat}$$
- 10/29 - So okay, looking at below, it's, what function can do that?
- 10/29 - Okay, confirmed, $\alpha_u$ determines $\alpha_d$, so just one variable: $$\begin{alignat}{2}
\ket{r}=\\{1.0, 0.5, 0.5\\}=f(\\{0.5, 0.5, 1.0\\}&, \\{0.5, 0.5, 0.0\\}, \alpha_r)\\\\
\ket{l}=\\{0.0, 0.5, 0.5\\}=f(\\{0.5, 0.5, 1.0\\}&, \\{0.5, 0.5, 0.0\\}, \alpha_l)\\\\
\ket{o}=\\{0.5, 1.0, 0.5\\}=f(\\{0.5, 0.5, 1.0\\}&, \\{0.5, 0.5, 0.0\\}, \alpha_o)\\\\
\ket{i}=\\{0.5, 0.0, 0.5\\}=f(\\{0.5, 0.5, 1.0\\}&, \\{0.5, 0.5, 0.0\\}, \alpha_i)\end{alignat}$$
- 10/29 - Maybe better yet: $$\begin{alignat}{2}
\ket{r}=\\{1.0, 0.5, 0.5\\}=f(\\{0.5, 0.5, 1.0\\}&, \\{0.5, 0.5, 0.0\\})\\\\
\ket{l}=\\{0.0, 0.5, 0.5\\}=f(\\{0.5, 0.5, 1.0\\}&, \\{0.5, 0.5, 0.0\\})\\\\
\ket{o}=\\{0.5, 1.0, 0.5\\}=f(\\{0.5, 0.5, 1.0\\}&, \\{0.5, 0.5, 0.0\\})\\\\
\ket{i}=\\{0.5, 0.0, 0.5\\}=f(\\{0.5, 0.5, 1.0\\}&, \\{0.5, 0.5, 0.0\\})\end{alignat}$$
- 10/29 - So actually, here's the _result_ we want, given some unknown operator $\bigoplus$: $$\begin{alignat}{2}
\ket{r}=\\{1.0, 0.5, 0.5\\}=\\{0.5, 0.5, 1.0\\}&\bigoplus\\{0.5, 0.5, 0.0\\}\\\\
\ket{l}=\\{0.0, 0.5, 0.5\\}=\\{0.5, 0.5, 1.0\\}&\bigoplus\\{0.5, 0.5, 0.0\\}\\\\
\ket{o}=\\{0.5, 1.0, 0.5\\}=\\{0.5, 0.5, 1.0\\}&\bigoplus\\{0.5, 0.5, 0.0\\}\\\\
\ket{i}=\\{0.5, 0.0, 0.5\\}=\\{0.5, 0.5, 1.0\\}&\bigoplus\\{0.5, 0.5, 0.0\\}\end{alignat}$$
- 10/28 - Maybe you'd _have_ to just introduce it like the book did‚Äîmagic. But that's precisely what I want to avoid. So let's try to avoid and come at it from our own way.
- 10/28 - The question is: Why would we put it in the linear additive form to begin with?

<b>older</b>

- Was thinking about mapping percentages to unit sphere, e.g. 70% = 0.7. And about how since we don't need to know the length, if we can express in polar coordinates and only use 2 angles. But stuck on how up and down can represent everything. Aren't they duplicating information? Obviously imaginary numbers are doing something. But think.
- The arrow that points has no magnitude; we only care about direction. You just need to know the tilting angle in 2 dimensions. 180 degrees in EITHER or BOTH means down. Put another way, with 2 degrees of freedom you can determine "how right" and "how out"‚Äîand then the rest can be determined as a difference from 100%.
- We can cover with just 2 angles, $\theta$ and $\phi$. $\theta$ can indicate how much to tilt "right"‚Äîat $\theta=90^\circ$ it is tilted completely right‚Äîat $\theta=0$ it is not tilted at all and so it faces up‚Äîat $\theta=180^\circ$ it is tilted completely and so it faces down. The same with $\phi$ and the direction out. So our situationship is just $(\theta, \phi)$ with 2 degrees of freedom. Let's see how we'd translate this to our previous notation of 3 percentages. What function $f(\theta)$ yields completely up (0% right), completely right (100% right), and completely down (0% right) at $\theta$s of $0^\circ$, $90^\circ$, and $180^\circ$? ... $(?, \sin\theta, \sin\phi)$.
- If you're wondering why not the 3 cartesian ... polar coordinates have an advantage‚Äîcan express a direction without length, which is impossible in cartesian.
- Now, where did trigonometrics come from? ... It's just a convenient choice. It's a function we've learned. It happens to be a function used everywhere. It's one of the most pristine functions for capturing periodicity, as we have in tilting. It's important to remember that we simply adopted an existing tool for convenience, because it chews up and spits out the numbers we agree with.
- So we've come up with several ways to notate situationships, each way more compact than the last. We ended up with a notation that captures the continuous range ... That is good and all, but $$\ket{A} = \alpha_u \\{0^\circ, 0^\circ\\} + \alpha_d \\{180^\circ, 180^\circ\\}$$ ... Say you wanted to represent $\ket{r}$ ... $$\\{90^\circ, 0^\circ\\} = \alpha_u \\{0^\circ, 0^\circ\\} + \alpha_d \\{180^\circ, 180^\circ\\}$$
- Well, we are not sure what to do with this.
- How _else_ can we represent situationships, such that we can do arithmetic with the up and down situationships to arrive at any other situationship?
- There turns out to be a clever way.
- Reset. Imagine. Even our current form, the 2 angles, have to be "translated" to probabilities via sin and cos.
- Reset. Someone figured out a clever way to represent situationships such that ... 1) Let us introduce "coefficients"‚Äîthey are the square root of the probability. Why a square root? No special reason other than it makes the math work and doing new math with it gives experimentally verifiable results. 2) We know for $\braket{u|r}$ the probability of measuring a $+1$ is $1/2$ so we say the coefficient is $1/\sqrt{2}$. Same for $\braket{d|r}$. 3) Now, we could do the same for all other combinations, making all the 50% probabilities yield a coefficient of $1/\sqrt{2}$. But here, we consider an alternate choice, to help make the arithmetic system work. The arithmetic system wants to assert that all situations can be represented as a linear formula, so for example $\ket{r}=\alpha_u\ket{u}+\alpha_d\ket{d}$ ... Why is $\braket{u|u}=1$?

TODO
- Why sometimes called the apparatus, why is the symbol needed, why sometimes called the device, etc.

Test.

</div>




we can say one situation is indistinguishable from another if the probabilities of u and d are the same
(so even l/r, i/o, correct, indistinguishable)


<!--
  What properties can a situation have?
but the property is just one and it's this: It's the **probability** of measuring a $+1$ versus a $-1$, were we to orient ùíú along the $z$-axis and take a measurement.
Given an ùíú prepared in an unknown manner (_i.e._ that's in an unknown situation), if we orient 
a way to expressed
situations
end up being measured
The answer is a refinement of things we've said before, and a [strange rule](#TODO) itself:
<blockquote><b>Any unknown situation $\ket{A}$ can be represented as a linear combination of the "up" and "down" known situations $\ket{u}$ and $\ket{d}$, $$\ket{A}=\alpha_u\ket{u}+\alpha_d\ket{d}\small\textrm{,}$$ where $\alpha_u$ is the probability* of </b></blockquote>
-->








<br>
<br>
<br>
<br>
<br>

---

1. sqrt(i) ... what if it was actually a 3rd dimension?




**maybe to be used:**
$\ket{A}$ means "state prepared in direction a"


3. so if we said it was prepared up, it would just be $\ket{u}$=au$\ket{u}$+ad$\ket{d}$ which forces au to be 1, ad to be 0
4. now let's say it's prepared right, we wanna find the coefficients
5. let's do a trick and multiply across by , whose meaning we do not know yet. we end up with a formula for each component.
6. this trick may not be necessary. we can just point at au in the original formula and say, that coefficient turns out to be square root of the probability of measuring its partner
7. but a special square root. we are just gonna tell you the coefficients are imaginary. why? because the intermediate steps wouldn't work unless they were
8. so if we were to prepare a state $\ket{r}$, then what probabilities should we expect to measuring $\ket{u}$ or $\ket{d}$? 50% each right? or 0.5. we said the coefficients are the square root of the probability. so, 1/sqrt(2) for each.
9. ALL we're saying so far, is not that $\ket{r}$ "IS" that formula, but that if we _only_ cared about the probabilities, forget eeeeeverything else, then the _only_ way this strange system can stay consistent is to make those coefficients 1/sqrt(2). will this result in a runnable and consistent experiment? no idea! we are in the midst of intermediate steps. "IMAGINARY" steps.


feels like... imaginary numbers just give us the ability to go 2D from 1D... just gives rise to dimensionality... why not just use dimensions?



$$|X\\rangle=\\alpha\_u|u\\rangle+\\alpha\_d|d\\rangle$$











---

# Notes to Authors / Teachers Notes (another article on the writing of this article)


https://www.physicsforums.com/threads/understanding-spin-states-in-2d-vector-spaces.1017499/post-6661598

> **_Very_ Optional Note:** This is where we break from the lecture. The lecture at this point imports several abstract rules. 1. It multiplies both sides of the above equation by the bra-vector ‚ü®u| or the bra-vector ‚ü®d|, whatever that's supposed to mean. 2. Then it imports a rule of bra-ket vectors to replace ‚ü®u$\ket{u}$ with 1 and ‚ü®u$\ket{d}$ with 0, teasing out an explicit formula for the coefficients Œ±<sub>u</sub> and Œ±<sub>d</sub>, for no apparent reason. 3. Then it asserts that these coefficients are complex numbers, and multiplies them by their own conjugates to get Œ±<sub>u</sub><sup>*</sup>Œ±<sub>u</sub> and Œ±<sub>d</sub><sup>*</sup>Œ±<sub>d</sub>. 4. Finally it tells us what these latest entities mean: They are the probabilities of measuring "up" or "down" after preparing the apparatus ùíú to be in the situation $\ket{A}$ that we began with. The book even admitted, _"These equations are extremely abstract, and it is not at all obvious what their physical significance is."_ But maybe it can be taught another way‚Äîthat's what we're about to try.

---

We begin constructing our arbitrary, typographic system by importing the concept of **vector spaces**. Again, why? Because it turns out to be a convenient representation. Here by the way, for those here without a formal background in the subject, the term "dimension" is more general than the notion used in _Flatland_ or _Hyperspace_; picturing two spin states as points on a graph, one might be tempted to say they lie on a 1-dimensional line, and that's not untrue, but that representation won't be as useful to us as considering each spin state a distinct dimension.

<sub>Note: This page uses LaTeX. Viewing it requires installing [TeX All the Things](https://chrome.google.com/webstore/detail/tex-all-the-things/cbimabofgmfdkicghcadidpemeenbffn?hl=en) on Chrome or running a MathJax userscript such as [MathJax for Reddit](chrome-extension://dhdgffkkebhmkfjojejmpbldmpobfkfo/ask.html?aid=78aa3c3f-9901-45d1-9c48-11309c39ed5a) but modified for _write.as_.</sub>


A note for overconcerned readers: In Lecture 1, ùíú referred to "the apparatus"‚Äîthis is not that. One might think we should have picked a different symbol in general for $A$, say $S$, to represent this generic State. But as we will see, the symbol $\alpha$ _does_ relate to $A$, as _coefficients to the components of vector $A$_. So $A$ may have been chosen due to $\alpha$ being a standard choice for coefficients, or because $A$ itself is a standard choice for generic states, or because in Lecture 1 we learned axioms for bras and kets using $A$, _e.g._ $\braket{B}{A}=\braket{A}{B}^*$, or maybe even because in a way we're _meant_ to connect our $A$ to the apparatus ùíú. Probably all of the above. Let's continue.

If we apply the corresponding up and down bras to both sides, we get
\begin{align}\braket{u}{A}=\bra{u}\alpha\_u\ket{u}=\alpha\_u\\\\\braket{d}{A}=\bra{d}\alpha\_d\ket{d}=\alpha\_d\end{align}
which is just another way of saying that the inner product of _some state_ and _a basis vector_ yields the coefficient for the component along that basis's direction. Why did we do that though? It's simply because we'll commonly use this substitution later.

Now, here's where it'll pay off to remember that we're constructing an arbitrary, typographic system that somehow generates true statements.

> "The components $\alpha\_u$ and $\alpha\_d$ are complex numbers."

Take this as an arbitrary rule. Lecture 1 did say that anything in the form $\braket{X}{Y}$, _i.e._ an inner product between a bra-vector and a ket-vector, results in a complex number. But that still doesn't explain _why_.

> "By themselves, they have no experimental meaning, but their magnitudes do."

It turns out that _the square of the magnitude \\(\alpha_u\\)_, _i.e._ \\(|\alpha_u|^2\\), maps back to something experimentally verifiable and consistent: the _probability_ of the spin being measured _up_.*

<sup>* Given a spin prepared in the state $\ket{A}$ and the apparatus oriented in the _up_-_down_ axis, yada yada.</sup>

\\(\alpha\\)

In the realm of complex numbers $\mathbb{C}$ (which includes real numbers $\mathbb{R}$), there's another way to write $|\alpha\_u|^2$. It can be written $\alpha\_u^\*\alpha\_d$, _i.e._ the product of $\alpha\_u$  and its _complex conjugate_. (It's simply an arithmetic equivalence‚Äîyou can try it out to see.) This form is preferred because complex conjugates play a role in the bra-ket system, namely in the axiom from Lecture 1 that $\braket{A}{B}=\braket{B}{A}^\*$. Applying this axiom after substituting the result from before that $\alpha\_u=\braket{u}{A}$, we find that the probability $P\_u$ can also be expressed as
$$P\_u=\braket{u}{A}\braket{A}{u}$$
And the same for $P\_d$ and all of the other basis vectors we'll encounter:
\begin{alignat}{3}
&P\_d&&=&&\braket{d}{A}\braket{A}{d}\\\\
&P\_l&&=&&\braket{l}{A}\braket{A}{l}\\\\
&P\_r&&=&&\braket{r}{A}\braket{A}{i}\\\\
&P\_i&&=&&\braket{i}{A}\braket{A}{r}\\\\
&P\_o&&=&&\braket{o}{A}\braket{A}{o}
\end{alignat}
Backing up a little, remember that value that had "no experimental meaning"? _i.e._ the coefficient $\alpha\_u$. Even though it has no (obvious) counterpart in our world, it will be useful to be able to refer to it by a name‚Äîwe call it a **probability amplitude**.

So far it has not been explicitly stated how to interpret the form $\braket{v}{A}$ in English. One way is, _"Given an apparatus whose spin was prepared in the state $\ket{A}$, now oriented toward the direction pointed by the basis vector $v$, $\braket{v}{A}$ is the probability that the apparatus will register $v$ when measured again."_

So for example,

> $$\braket{u}{d}=0$$

is just saying, _"Given an apparatus whose spin was prepared as $d$, now oriented toward $u$, the probability of the apparatus registering $u$ when measured again is 0."_

Whereas

> $$\braket{A}{A}=1$$

is saying, _"Given an apparatus whose spin was prepared as $A$, now oriented toward the direction that $A$ was measured to be, the probability of the apparatus registering $A$ when measured again is 1."_

<div class="references">
  <p>References:</p>
  <ol>
    <li>PeroK. (2022, August 9). <i>Understanding spin states in 2D vector spaces</i>. Physics Forums. https://www.physicsforums.com/threads/1017499/.</li>
  </ol>
</div>

HEREDOC; include '_article.html'; ?>
